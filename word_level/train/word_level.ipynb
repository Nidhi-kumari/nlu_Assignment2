{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vikas\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = ['whitman-leaves.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt','shakespeare-macbeth.txt']\n",
    "with open('input.txt', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29166\n"
     ]
    }
   ],
   "source": [
    "num_lines = sum(1 for line in open('input.txt'))\n",
    "print(num_lines)\n",
    "#split data into train test and validation\n",
    "\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "\n",
    "i=int(num_lines*0.7)\n",
    "\n",
    "train_data = data[:i]\n",
    "test_data = data[i:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "806883"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "thefile = open('train.txt', 'w')\n",
    "thefile.write(\"\\n\".join(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 139632\n",
      "Unique Tokens: 15297\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    " \n",
    "\n",
    "def load_doc(filename):\n",
    "        \n",
    "        file = open(filename, 'r')\n",
    "        # read all text\n",
    "        text = file.read()\n",
    "        # close the file\n",
    "        file.close()\n",
    "        return text\n",
    "\n",
    "\n",
    "def clean_doc(doc):\n",
    "        \n",
    "        doc = doc.replace('--', ' ')\n",
    "        \n",
    "        tokens = doc.split()\n",
    "        \n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        \n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        \n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        return tokens\n",
    " \n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "        data = '\\n'.join(lines)\n",
    "        file = open(filename, 'w')\n",
    "        file.write(data)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "in_filename = 'train.txt'#\n",
    "doc = load_doc(in_filename)\n",
    "\n",
    " \n",
    "\n",
    "tokens = clean_doc(doc)\n",
    "#print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 139581\n"
     ]
    }
   ],
   "source": [
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    " \n",
    "# save sequences to file\n",
    "out_filename = 'sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of  Sequences 139581\n",
      "Vocabulary size : 15298\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#load\n",
    "in_filename = 'sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "#print(type(lines))\n",
    "#print(lines[:5])\n",
    " \n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    " \n",
    "# separate into input and output\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "\n",
    "print(\"Total no of  Sequences\",len(sequences))\n",
    "#tokenizer = Tokenizer()\n",
    "#tokenizer.fit_on_texts(sequences)\n",
    "#sequences = tokenizer.texts_to_sequences(sequences)\n",
    "#vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size :',vocab_size)\n",
    "#sequences=np.array(sequences)\n",
    "#x,y=sequences[:,:-1],sequences[:,-1]\n",
    "#seq_length = x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            764900    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 15298)             1545098   \n",
      "=================================================================\n",
      "Total params: 2,460,898\n",
      "Trainable params: 2,460,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "139581/139581 [==============================] - 1256s 9ms/step - loss: 6.7826 - acc: 0.0760\n",
      "Epoch 2/30\n",
      "139581/139581 [==============================] - 1141s 8ms/step - loss: 6.6737 - acc: 0.0818\n",
      "Epoch 3/30\n",
      "139581/139581 [==============================] - 1161s 8ms/step - loss: 6.6102 - acc: 0.0857\n",
      "Epoch 4/30\n",
      "139581/139581 [==============================] - 1153s 8ms/step - loss: 6.5644 - acc: 0.0870\n",
      "Epoch 5/30\n",
      "139581/139581 [==============================] - 830s 6ms/step - loss: 6.5275 - acc: 0.0878\n",
      "Epoch 6/30\n",
      "139581/139581 [==============================] - 641s 5ms/step - loss: 6.4906 - acc: 0.0886\n",
      "Epoch 7/30\n",
      "139581/139581 [==============================] - 640s 5ms/step - loss: 6.4496 - acc: 0.0892\n",
      "Epoch 8/30\n",
      "139581/139581 [==============================] - 635s 5ms/step - loss: 6.4051 - acc: 0.0930\n",
      "Epoch 9/30\n",
      "139581/139581 [==============================] - 645s 5ms/step - loss: 6.3615 - acc: 0.0974\n",
      "Epoch 10/30\n",
      "139581/139581 [==============================] - 650s 5ms/step - loss: 6.3220 - acc: 0.1026\n",
      "Epoch 11/30\n",
      "139581/139581 [==============================] - 640s 5ms/step - loss: 6.2811 - acc: 0.1054\n",
      "Epoch 12/30\n",
      "139581/139581 [==============================] - 637s 5ms/step - loss: 6.2391 - acc: 0.1063\n",
      "Epoch 13/30\n",
      "139581/139581 [==============================] - 639s 5ms/step - loss: 6.1992 - acc: 0.1079\n",
      "Epoch 14/30\n",
      "139581/139581 [==============================] - 638s 5ms/step - loss: 6.1641 - acc: 0.1095\n",
      "Epoch 15/30\n",
      "139581/139581 [==============================] - 639s 5ms/step - loss: 6.1280 - acc: 0.1122\n",
      "Epoch 16/30\n",
      "139581/139581 [==============================] - 637s 5ms/step - loss: 6.0924 - acc: 0.1153\n",
      "Epoch 17/30\n",
      "139581/139581 [==============================] - 636s 5ms/step - loss: 6.0570 - acc: 0.1170\n",
      "Epoch 18/30\n",
      "139581/139581 [==============================] - 638s 5ms/step - loss: 6.0222 - acc: 0.1188\n",
      "Epoch 19/30\n",
      "139581/139581 [==============================] - 637s 5ms/step - loss: 5.9880 - acc: 0.1209\n",
      "Epoch 20/30\n",
      "139581/139581 [==============================] - 642s 5ms/step - loss: 5.9534 - acc: 0.1230\n",
      "Epoch 21/30\n",
      "139581/139581 [==============================] - 640s 5ms/step - loss: 5.9217 - acc: 0.1244\n",
      "Epoch 22/30\n",
      "139581/139581 [==============================] - 639s 5ms/step - loss: 5.8930 - acc: 0.1260\n",
      "Epoch 23/30\n",
      "139581/139581 [==============================] - 638s 5ms/step - loss: 5.8619 - acc: 0.1275\n",
      "Epoch 24/30\n",
      "139581/139581 [==============================] - 639s 5ms/step - loss: 5.8191 - acc: 0.1303\n",
      "Epoch 25/30\n",
      "139581/139581 [==============================] - 637s 5ms/step - loss: 5.7827 - acc: 0.1328\n",
      "Epoch 26/30\n",
      "139581/139581 [==============================] - 637s 5ms/step - loss: 5.7483 - acc: 0.1342\n",
      "Epoch 27/30\n",
      "139581/139581 [==============================] - 639s 5ms/step - loss: 5.8748 - acc: 0.1296\n",
      "Epoch 28/30\n",
      "139581/139581 [==============================] - 637s 5ms/step - loss: 5.8836 - acc: 0.1257\n",
      "Epoch 29/30\n",
      "139581/139581 [==============================] - 637s 5ms/step - loss: 5.8021 - acc: 0.1305\n",
      "Epoch 30/30\n",
      "139581/139581 [==============================] - 644s 5ms/step - loss: 5.7217 - acc: 0.1342\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=1000, epochs=30)\n",
    " \n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279873"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thefile1 = open('test.txt', 'w')\n",
    "thefile1.write(\"\\n\".join(test_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#texword sequences\n",
    "in_filename = 'test.txt'#\n",
    "doc = load_doc(in_filename)\n",
    "#print(doc[:200])\n",
    " \n",
    "# clean document\n",
    "tokens_test = clean_doc(doc)\n",
    "#print(tokens[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 50152\n"
     ]
    }
   ],
   "source": [
    "length = 50 + 1\n",
    "sequences_test = list()\n",
    "for i in range(length, len(tokens_test)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens_test[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences_test.append(line)\n",
    "print('Total Sequences: %d' % len(sequences_test))\n",
    " \n",
    "# save sequences to file\n",
    "out_filename = 'sequences_test.txt'\n",
    "save_doc(sequences_test, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "in_filename = 'sequences_test.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = list(doc.split('\\n'))\n",
    "\n",
    "\n",
    "model = load_model('model.h5')\n",
    "\n",
    "sequences_c = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "sequences = []\n",
    "for sequence in  sequences_c:\n",
    "        if(len(sequence)==51):\n",
    "            sequences.append(sequence)\n",
    "\n",
    "\n",
    "\n",
    "sequences =  np.asarray(sequences)\n",
    "\n",
    "X_test, y_test = sequences[:,:-1], sequences[:,-1]\n",
    "\n",
    "y_test = to_categorical(y_test, num_classes=vocab_size)\n",
    " \n",
    "\n",
    "loss = model.evaluate(X_test,y_test,batch_size=128, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity = 221.40641620418717\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "print(\"perplexity = {}\" .format(math.exp(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
